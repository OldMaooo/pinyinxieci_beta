#!/usr/bin/env python3
"""
è¯æ±‡è¡¨éªŒè¯å·¥å…·
ç”¨äºæ£€æŸ¥ç”Ÿæˆçš„è¯åº“æ•°æ®æ˜¯å¦ç¬¦åˆè§„èŒƒ

ä½¿ç”¨æ–¹æ³•:
    python verify_vocabulary.py [json_file]

ç¤ºä¾‹:
    python verify_vocabulary.py dist/data/ä¸€å¹´çº§ä¸Šå†Œ.json
"""

import json
import sys
from collections import defaultdict
from pathlib import Path


def load_json(file_path):
    """åŠ è½½JSONæ–‡ä»¶"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


def check_duplicates(data):
    """æ£€æŸ¥é‡å¤è¯"""
    word_count = defaultdict(int)
    unit_word_count = defaultdict(lambda: defaultdict(int))

    for item in data["wordBank"]:
        word = item["word"]
        unit = str(item["unit"])

        word_count[word] += 1
        unit_word_count[unit][word] += 1

    duplicates = {}
    for word, count in word_count.items():
        if count > 1:
            duplicates[word] = count

    return duplicates, unit_word_count


def check_suspicious_words(data):
    """æ£€æŸ¥å¯ç–‘è¯æ±‡ï¼ˆå¯èƒ½æ˜¯OCRé”™è¯¯ï¼‰"""
    # å¸¸è§å¯ç–‘æ¨¡å¼
    suspicious_patterns = [
        r"^.[å‘±å­åƒå’±å‘å“ª]$",  # å•å­—+ç”Ÿåƒ»åç¼€
        r"^[å¤©åœ°äººä½ æˆ‘ä»–é‡‘æœ¨æ°´ç«åœŸå£è€³ç›®æ—¥æœˆæ°´ç«]$",  # å•å­—ï¼ˆåº”è¯¥æ˜¯åŒå­—è¯ï¼‰
    ]

    suspicious = []
    for item in data["wordBank"]:
        word = item["word"]
        # æ£€æŸ¥è¿‡çŸ­çš„è¯
        if len(word) < 2:
            suspicious.append(
                {"word": word, "unit": item["unit"], "reason": "è¯å¤ªçŸ­ï¼ˆå¯èƒ½æ˜¯å•å­—ï¼‰"}
            )

    return suspicious


def check_units(data):
    """æ£€æŸ¥å•å…ƒå®Œæ•´æ€§"""
    units = set()
    for item in data["wordBank"]:
        units.add(str(item["unit"]))

    # æœŸæœ›çš„å•å…ƒåˆ—è¡¨ï¼ˆå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    expected_units = [
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "è¯­æ–‡å›­åœ°ä¸€",
        "è¯­æ–‡å›­åœ°äºŒ",
        "è¯­æ–‡å›­åœ°ä¸‰",
        "è¯­æ–‡å›­åœ°å››",
        "è¯­æ–‡å›­åœ°äº”",
        "è¯­æ–‡å›­åœ°å…­",
        "è¯­æ–‡å›­åœ°ä¸ƒ",
        "è¯­æ–‡å›­åœ°å…«",
    ]

    found_units = units - {
        "è¯­æ–‡å›­åœ°ä¸€",
        "è¯­æ–‡å›­åœ°äºŒ",
        "è¯­æ–‡å›­åœ°ä¸‰",
        "è¯­æ–‡å›­åœ°å››",
        "è¯­æ–‡å›­åœ°äº”",
        "è¯­æ–‡å›­åœ°å…­",
        "è¯­æ–‡å›­åœ°ä¸ƒ",
        "è¯­æ–‡å›­åœ°å…«",
    }

    return found_units


def analyze_word_sources(data):
    """åˆ†æè¯æ±‡æ¥æº"""
    unit_stats = {}

    for item in data["wordBank"]:
        unit = str(item["unit"])
        if unit not in unit_stats:
            unit_stats[unit] = {"count": 0, "words": []}
        unit_stats[unit]["count"] = unit_stats[unit]["count"] + 1
        unit_stats[unit]["words"].append(item["word"])

    return unit_stats


def generate_report(file_path):
    """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
    print(f"\n{'=' * 60}")
    print(f"è¯æ±‡è¡¨éªŒè¯æŠ¥å‘Š")
    print(f"{'=' * 60}")
    print(f"æ–‡ä»¶: {file_path}\n")

    try:
        data = load_json(file_path)
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        return

    # åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“š åŸºæœ¬ä¿¡æ¯:")
    print(f"   å¹´çº§: {data.get('gradeSemester', 'æœªçŸ¥')}")
    print(f"   è¯æ±‡æ€»æ•°: {data.get('count', len(data['wordBank']))}")
    print(f"   å®é™…è¯æ±‡æ•°: {len(data['wordBank'])}")

    # æ£€æŸ¥é‡å¤è¯
    print(f"\n{'=' * 60}")
    print(f"ğŸ” é‡å¤è¯æ£€æŸ¥")
    print(f"{'=' * 60}")

    duplicates, unit_words = check_duplicates(data)
    if duplicates:
        print(f"âŒ å‘ç° {len(duplicates)} ä¸ªé‡å¤è¯:\n")
        for word, count in sorted(duplicates.items(), key=lambda x: -x[1]):
            units = [u for u, words in unit_words.items() if word in words]
            print(f"   '{word}' - å‡ºç° {count} æ¬¡")
            print(f"      æ‰€åœ¨å•å…ƒ: {', '.join(units)}")
    else:
        print(f"âœ… æ— é‡å¤è¯")

    # æ£€æŸ¥å¯ç–‘è¯æ±‡
    print(f"\n{'=' * 60}")
    print(f"âš ï¸  å¯ç–‘è¯æ±‡æ£€æŸ¥")
    print(f"{'=' * 60}")

    suspicious = check_suspicious_words(data)
    if suspicious:
        print(f"âš ï¸  å‘ç° {len(suspicious)} ä¸ªå¯ç–‘è¯æ±‡:\n")
        for item in suspicious[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            print(f"   '{item['word']}' (å•å…ƒ{item['unit']}) - {item['reason']}")
        if len(suspicious) > 20:
            print(f"   ... è¿˜æœ‰ {len(suspicious) - 20} ä¸ª")
    else:
        print(f"âœ… æœªå‘ç°å¯ç–‘è¯æ±‡")

    # å•å…ƒç»Ÿè®¡
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š å•å…ƒç»Ÿè®¡")
    print(f"{'=' * 60}")

    stats = analyze_word_sources(data)
    print(f"å…±æœ‰ {len(stats)} ä¸ªå•å…ƒ:\n")

    for unit, info in sorted(
        stats.items(),
        key=lambda x: (not x[0].isdigit(), int(x[0]) if x[0].isdigit() else 999, x[0]),
    ):
        print(f"   å•å…ƒ {unit}: {info['count']} ä¸ªè¯")

    # æ€»ç»“
    print(f"\n{'=' * 60}")
    print(f"ğŸ“ æ€»ç»“")
    print(f"{'=' * 60}")

    issues = []
    if duplicates:
        issues.append(f"æœ‰ {len(duplicates)} ä¸ªé‡å¤è¯")
    if suspicious:
        issues.append(f"æœ‰ {len(suspicious)} ä¸ªå¯ç–‘è¯æ±‡")

    if issues:
        print(f"âŒ éœ€è¦ä¿®å¤: {', '.join(issues)}")
    else:
        print(f"âœ… è¯åº“è´¨é‡è‰¯å¥½")

    print(f"\n{'=' * 60}\n")


def main():
    if len(sys.argv) < 2:
        # é»˜è®¤æ£€æŸ¥ dist/data ç›®å½•ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶
        data_dir = Path("dist/data")
        if data_dir.exists():
            json_files = list(data_dir.glob("*ä¸Šå†Œ.json")) + list(
                data_dir.glob("*ä¸‹å†Œ.json")
            )
            for f in sorted(json_files):
                generate_report(f)
        else:
            print("è¯·æä¾› JSON æ–‡ä»¶è·¯å¾„")
            print("ç”¨æ³•: python verify_vocabulary.py <json_file>")
    else:
        generate_report(sys.argv[1])


if __name__ == "__main__":
    main()
